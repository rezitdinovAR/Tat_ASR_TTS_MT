{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split filelist file into train and test sets\n",
    "\n",
    "Use a train ratio or number of samples in test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  spkidx  spkid   txtid   uttid  \\\n",
      "0           0       0      1   i2643   i2643   \n",
      "1           1       0      1   i2124   i2124   \n",
      "2           2       0      1    i574    i574   \n",
      "3           3       0      1  i02299  i02299   \n",
      "4           4       0      1    i248    i248   \n",
      "\n",
      "                                                text  \\\n",
      "0  аны сәяси совет секретаре тр дәүләт советы рәи...   \n",
      "1  аны зурга күтәреп җитди эш дип саныйсы да юк  ...   \n",
      "2                      аңардан сак булырга кирәк иде   \n",
      "3  ләкин сез күз алдыгызга китерә аласызмы минем ...   \n",
      "4  стариков белән газинур аның һәр кул хәрәкәтен ...   \n",
      "\n",
      "                                            phonemes  \n",
      "0  ɒnˈɯ sæjæsˈi sowˈet sekretɑrˈe tˌeˈer dæwlˈæt ...  \n",
      "1  ɒnˈɯ zurʁˈɑ kytærˈep ʑitdˈi ˈeʃ dˈip sɒnɯjsˈɯ ...  \n",
      "2                 ɒŋɒrdˈɑn sˈɒq buɫɯrʁˈɑ kirˈæk idˈe  \n",
      "3  lækˈin sˈez kˈyz ɒɫdɯʁɯzʁˈɑ kiterˈæ ɒɫɒsɯzmˈɯ ...  \n",
      "4  stɒriqˈow belˈæn ʁɒzinˈur ɒnˈɯŋ hˈær qˈuɫ xæræ...  \n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the csv file\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "dataset_name = \"Almaz_new\"\n",
    "data: pd.DataFrame = pd.read_csv(f\"../filelists/{dataset_name}.csv\",sep='\\t')\n",
    "#data = data.drop(['Unnamed: 0.3','Unnamed: 0.1','Unnamed: 0.2','Unnamed: 0'],axis=1)\n",
    "#data['phonemes'] = data['phoneme']\n",
    "#data = data.drop(['phoneme'],axis=1)\n",
    "print(data.head())\n",
    "print(type(data['uttid'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.loc[data['uttid'] == 1462]\n",
    "#data = data.drop([3107,3481],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support for DataFrames\n",
    "def split_file_list(orig_data: pd.DataFrame, train_ratio=None, test_samples=None, max_samples=None):\n",
    "    # Shuffle the data\n",
    "    data = orig_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    if max_samples is not None:\n",
    "        data = data[:max_samples]\n",
    "\n",
    "    if test_samples is not None:\n",
    "        train_set = data[:-test_samples]\n",
    "        test_set = data[-test_samples:]\n",
    "    elif train_ratio is not None:\n",
    "        train_set_size = int(len(data) * train_ratio)\n",
    "        train_set = data[:train_set_size]\n",
    "        test_set = data[train_set_size:]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Either 'train_ratio' or 'test_samples' should be provided.\")\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "# Example usage\n",
    "train_data, val_data = split_file_list(data, test_samples=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save phonemes and text of train_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_dir = \"home/ips/projects/tatar-tts-2/VITS/vits/Almaz\"\n",
    "o_file_train = f\"../filelists/{dataset_name}_audio_sid_text_train_filelist.txt\"\n",
    "o_file_val = f\"../filelists/{dataset_name}_audio_sid_text_test_filelist.txt\"\n",
    "\n",
    "link_name = \"DUMMY5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_map(source_dir):\n",
    "    path_map = {}\n",
    "    for files in os.walk(source_dir):\n",
    "        !pwd\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                path_map[file] = os.path.join(root, file)\n",
    "                print(path_map[file])\n",
    "    return path_map\n",
    "\n",
    "\n",
    "def save_file_list(data, out_file_path, source_dir, path_map, link_name, cleaned_text=False):\n",
    "    with open(out_file_path, \"w\") as file:\n",
    "        for row in data.itertuples():\n",
    "            uttid = f\"{row.uttid}.wav\"\n",
    "            path = path_map[uttid].replace(source_dir, link_name)\n",
    "            spkidx = row.spkidx\n",
    "            info = row.text if not cleaned_text else row.phonemes\n",
    "\n",
    "            file.write(f\"{path}|{spkidx}|{info}\\n\")\n",
    "            # Print every nth sample\n",
    "            if row.Index % 5000 == 0:\n",
    "                print(f\"{row.Index}: {path}|{spkidx}|{info}\")\n",
    "\n",
    "    print(f\"Saved to '{out_file_path}' ({len(data)} samples).\")\n",
    "\n",
    "\n",
    "def save_files(data, out_file_path, source_dir, path_map, link_name):\n",
    "    save_file_list(train_data, out_file_path, source_dir, path_map, link_name)\n",
    "    if \"phonemes\" in data.columns:\n",
    "        out_file_path = out_file_path.replace(\".txt\", \".txt.cleaned\")\n",
    "        save_file_list(data, out_file_path, source_dir,\n",
    "                       path_map, link_name, cleaned_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_map = {}\n",
    "for files in os.walk(i_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            path_map[file] = os.path.join(root, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "path_map = create_path_map(i_dir)\n",
    "print(path_map)\n",
    "\n",
    "#save_files(train_data, o_file_train, i_dir, path_map, link_name)\n",
    "#save_files(val_data, o_file_val, i_dir, path_map, link_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a symlink to the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create symlink to the dataset\n",
    "!ln -s {i_dir} {link_name}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kai",
   "language": "python",
   "name": "kai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

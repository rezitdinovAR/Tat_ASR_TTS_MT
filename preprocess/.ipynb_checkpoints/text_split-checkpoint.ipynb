{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split filelist file into train and test sets\n",
    "\n",
    "Use a train ratio or number of samples in test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 \\tspkidx\\tspkid\\ttxtid\\tuttid\\ttext\n",
      "0  0\\t0\\t1\\t02551\\t02551\\tүзе кисәк кенә борылып ...\n",
      "1  1\\t0\\t1\\t02705\\t02705\\tзөһрә ханымның бәхете  ...\n",
      "2  2\\t0\\t1\\t00308\\t00308\\tәйе аның кебек чибәр ха...\n",
      "3  3\\t0\\t1\\t666\\t666\\tтычканны җитештерүче леново...\n",
      "4  4\\t0\\t1\\t200\\t200\\tсатып алыгыз да инде уралец...\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the csv file\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "dataset_name = \"Almaz_new\"\n",
    "data: pd.DataFrame = pd.read_csv(f\"../filelists/{dataset_name}.csv\",sep='\\t')\n",
    "data \n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support for DataFrames\n",
    "def split_file_list(orig_data: pd.DataFrame, train_ratio=None, test_samples=None, max_samples=None):\n",
    "    # Shuffle the data\n",
    "    data = orig_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    if max_samples is not None:\n",
    "        data = data[:max_samples]\n",
    "\n",
    "    if test_samples is not None:\n",
    "        train_set = data[:-test_samples]\n",
    "        test_set = data[-test_samples:]\n",
    "    elif train_ratio is not None:\n",
    "        train_set_size = int(len(data) * train_ratio)\n",
    "        train_set = data[:train_set_size]\n",
    "        test_set = data[train_set_size:]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Either 'train_ratio' or 'test_samples' should be provided.\")\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "\n",
    "# Example usage\n",
    "train_data, val_data = split_file_list(data, test_samples=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save phonemes and text of train_data, val_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_dir = \"path/to/wav/directory\"\n",
    "o_file_train = f\"../filelists/{dataset_name}_audio_sid_text_train_filelist.txt\"\n",
    "o_file_val = f\"../filelists/{dataset_name}_audio_sid_text_test_filelist.txt\"\n",
    "\n",
    "link_name = \"DUMMY3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_map(source_dir):\n",
    "    path_map = {}\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                path_map[file] = os.path.join(root, file)\n",
    "    return path_map\n",
    "\n",
    "\n",
    "def save_file_list(data, out_file_path, source_dir, path_map, link_name, cleaned_text=False):\n",
    "    with open(out_file_path, \"w\") as file:\n",
    "        for row in data.itertuples():\n",
    "            uttid = f\"{row.uttid}.wav\"\n",
    "            path = path_map[uttid].replace(source_dir, link_name)\n",
    "            spkidx = row.spkidx\n",
    "            info = row.text if not cleaned_text else row.phonemes\n",
    "\n",
    "            file.write(f\"{path}|{spkidx}|{info}\\n\")\n",
    "            # Print every nth sample\n",
    "            if row.Index % 5000 == 0:\n",
    "                print(f\"{row.Index}: {path}|{spkidx}|{info}\")\n",
    "\n",
    "    print(f\"Saved to '{out_file_path}' ({len(data)} samples).\")\n",
    "\n",
    "\n",
    "def save_files(data, out_file_path, source_dir, path_map, link_name):\n",
    "    save_file_list(train_data, out_file_path, source_dir, path_map, link_name)\n",
    "    if \"phonemes\" in data.columns:\n",
    "        out_file_path = out_file_path.replace(\".txt\", \".txt.cleaned\")\n",
    "        save_file_list(data, out_file_path, source_dir,\n",
    "                       path_map, link_name, cleaned_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_map = create_path_map(i_dir)\n",
    "\n",
    "\n",
    "save_files(train_data, o_file_train, i_dir, path_map, link_name)\n",
    "save_files(val_data, o_file_val, i_dir, path_map, link_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a symlink to the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create symlink to the dataset\n",
    "!ln -s {i_dir} {link_name}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kai",
   "language": "python",
   "name": "kai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
